\relax 
\@nameuse{bbl@beforestart}
\citation{Shai_2015}
\citation{Shai_2015}
\citation{Shai_2015}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Dendritic Computation for Sequence Prediction}{1}\protected@file@percent }
\newlabel{eq:simpl_prox_dist}{{1}{1}}
\newlabel{eq:sigmoidal}{{2}{1}}
\newlabel{eq:single_neur_0}{{3}{1}}
\newlabel{eq:single_neur_1}{{4}{1}}
\newlabel{eq:single_neur_2}{{5}{1}}
\newlabel{eq:single_neur_3}{{6}{1}}
\newlabel{eq:single_neur_4}{{7}{1}}
\newlabel{eq:single_neur_5}{{8}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Firing rate as a function of distal and proximal input of the rate model proposed in \cite  {Shai_2015}.}}{2}\protected@file@percent }
\newlabel{fig:Shai_prox_dist}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Output firing rate as a function of proximal and distal input as given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:simpl_prox_dist}\unskip \@@italiccorr )}}}}{2}\protected@file@percent }
\newlabel{fig:simpl_prox_dist}{{2}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter settings for the setup described in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:single_neur_0}\unskip \@@italiccorr )}} -- \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:single_neur_5}\unskip \@@italiccorr )}}.}}{2}\protected@file@percent }
\newlabel{tab:single_neuron_parameters}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A single neuron receiving multiple proximal inputs and a single distal signal.}}{3}\protected@file@percent }
\newlabel{fig:single_neuron_illustration}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Weights and input signals before and after Hebbian learning of proximal weights, using nonlinear proximal-distal interaction.}}{3}\protected@file@percent }
\newlabel{fig:single_neuron_results_1}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Weights and input signals before and after Hebbian learning of proximal weights, using linear proximal-distal summation.}}{4}\protected@file@percent }
\newlabel{fig:single_neuron_results_2}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Same setup as in Fig. \ref  {fig:single_neuron_results_2}, but with the signal of the second proximal input channel scaled by a factor of two.}}{5}\protected@file@percent }
\newlabel{fig:single_neuron_results_3}{{6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Output Dynamics After Learning}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Same setup as in Fig. \ref  {fig:single_neuron_results_1}, but with the signal of the second proximal input channel scaled by a factor of two.}}{6}\protected@file@percent }
\newlabel{fig:single_neuron_results_4}{{7}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Output activity for a set of constant distal inputs (red to blue) applied after learning. Green line corresponds to the distal input also used for the learning phase.}}{7}\protected@file@percent }
\newlabel{fig:const_dist_sweep_mod}{{8}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Analytic Approximation of Weight Dynamics}{7}\protected@file@percent }
\newlabel{eq:simpl_further_prox_dist}{{9}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Output activity for a set of fluctuating distal inputs (red to blue) that are uncorrelated with the proximal input (applied after learning). Green line corresponds to the distal input also used for the learning phase.}}{8}\protected@file@percent }
\newlabel{fig:altern_dist_sweep_mod}{{9}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Activity for independently fluctuating values of $I_{\rm  d}$ and $I_{\rm  p}$ both ranging from $0$ to $1$, projected onto $I_{\rm  d}$ (left) and $I_{\rm  p}$ (right).}}{8}\protected@file@percent }
\newlabel{fig:output_mod_project}{{10}{8}}
\newlabel{eq:approx_delta_w}{{12}{9}}
\newlabel{eq:mse_diff}{{16}{9}}
\newlabel{eq:stab_cond_w}{{19}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Effect of ``Distraction" Components in the Proximal Input}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Pattern Classification Task}{10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Normalized mean squared error between $\mathbf  {w}^{\rm  p}$ and vector $\mathbf  {a}$ defining the linear combination of signals presented to the distal input for a given distraction scaling $s$.}}{11}\protected@file@percent }
\newlabel{fig:distraction_scaling}{{11}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Binary classification accuracy for different amounts of training patterns and different neuron models.}}{12}\protected@file@percent }
\newlabel{fig:class_accuracy}{{12}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Binary classification accuracy for different amounts of training patterns and different neuron models. Same setup as in Fig.~\ref  {fig:class_accuracy}, but allowing for negative weights in the point and compartment model}}{13}\protected@file@percent }
\newlabel{fig:class_accuracy_no_constraint}{{13}{13}}
\bibstyle{unsrt}
\bibdata{/home/fabian/work/lit_base.bib}
\bibcite{Shai_2015}{{1}{}{{}}{{}}}
\bibcite{Urbanczik_2014}{{2}{}{{}}{{}}}
\bibcite{Barbas_2015}{{3}{}{{}}{{}}}
\bibcite{Larkum_2018}{{4}{}{{}}{{}}}
\bibcite{Bastos_2012}{{5}{}{{}}{{}}}
\bibcite{Meyer_2011}{{6}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Binary classification accuracy for different amounts of training patterns and different neuron models. Same setup as in Fig.~\ref  {fig:class_accuracy}, but with class $0$ appearing with $90\%$ probability (as opposed to a $50/50$ chance as in the previous setups).}}{14}\protected@file@percent }
\newlabel{fig:class_accuracy_imbalance}{{14}{14}}
\bibcite{Sjoestroem_2006}{{7}{}{{}}{{}}}
\bibcite{Letzkus_2006}{{8}{}{{}}{{}}}
\bibcite{Gambino_2014}{{9}{}{{}}{{}}}
\bibcite{Williams_2010}{{10}{}{{}}{{}}}
\bibcite{Wibral_2017}{{11}{}{{}}{{}}}
\bibcite{Kay_2019}{{12}{}{{}}{{}}}
\bibcite{McGill_1954}{{13}{}{{}}{{}}}
\bibcite{Moldwin_2020}{{14}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
