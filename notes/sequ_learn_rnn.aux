\relax 
\citation{Rumelhart_1988,Elman_1990}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Elman_1990,Jordan_1989}
\citation{Friston_2005}
\citation{OReilly_2014}
\citation{Clopath_Bono_2017}
\citation{Shai_2015}
\citation{Jaeger_2010}
\citation{Shai_2015}
\citation{Shai_2015}
\citation{Shai_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the BPTT algorithm.}}{2}}
\newlabel{fig:BPTT}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Backpropagation Through Time}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Possible Mechanism of Sequence Learning in the Brain}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Echo-State Networks}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Firing rate as a function of distal and proximal input of the rate model proposed in \cite  {Shai_2015}.}}{3}}
\newlabel{fig:Shai_prox_dist}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dendritic Computation for Sequence Prediction}{3}}
\newlabel{eq:simpl_prox_dist}{{6}{3}}
\newlabel{eq:sigmoidal}{{7}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Output firing rate as a function of proximal and distal input as given by \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:simpl_prox_dist}\unskip \@@italiccorr )}}}}{4}}
\newlabel{fig:simpl_prox_dist}{{3}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameter settings for the setup described in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:single_neur_0}\unskip \@@italiccorr )}} -- \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:single_neur_5}\unskip \@@italiccorr )}}.}}{4}}
\newlabel{tab:single_neuron_parameters}{{1}{4}}
\newlabel{eq:single_neur_0}{{8}{4}}
\newlabel{eq:single_neur_1}{{9}{4}}
\newlabel{eq:single_neur_2}{{10}{4}}
\newlabel{eq:single_neur_3}{{11}{4}}
\newlabel{eq:single_neur_4}{{12}{4}}
\newlabel{eq:single_neur_5}{{13}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A single neuron receiving multiple proximal inputs and a single distal signal.}}{5}}
\newlabel{fig:single_neuron_illustration}{{4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Weights and input signals before and after Hebbian learning of proximal weights, using nonlinear proximal-distal interaction.}}{5}}
\newlabel{fig:single_neuron_results_1}{{5}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Weights and input signals before and after Hebbian learning of proximal weights, using linear proximal-distal summation.}}{6}}
\newlabel{fig:single_neuron_results_2}{{6}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Analytic Approximation of Weight Dynamics}{6}}
\newlabel{eq:simpl_further_prox_dist}{{14}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Same setup as in Fig. \ref  {fig:single_neuron_results_2}, but with the signal of the second proximal input channel scaled by a factor of three.}}{7}}
\newlabel{fig:single_neuron_results_3}{{7}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Same setup as in Fig. \ref  {fig:single_neuron_results_1}, but with the signal of the second proximal input channel scaled by a factor of three.}}{7}}
\newlabel{fig:single_neuron_results_4}{{8}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of weight dynamics between analytic approximation and full simulation.}}{8}}
\newlabel{fig:weight_dyn_analytic_comp}{{9}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Adding Plasticity to the Distal Input}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Proximal and distal inputs generated by chaotic sequences ({\bf  A},{\bf  B}) and the corresponding weight dynamics ({\bf  C},{\bf  D})}}{9}}
\newlabel{fig:dist_plast_comp}{{10}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Application to a Recurrent Network}{9}}
\bibstyle{unsrt}
\bibdata{/home/fschubert/work/lit_base.bib}
\bibcite{Rumelhart_1988}{{1}{}{{}}{{}}}
\bibcite{Elman_1990}{{2}{}{{}}{{}}}
\bibcite{Jordan_1989}{{3}{}{{}}{{}}}
\bibcite{Friston_2005}{{4}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Combining correlation-sensitive learning with a recurrent reservoir.}}{10}}
\newlabel{fig:prox_dist_recurrent}{{11}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Test with a Gradient Descent Rule}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Proximal and distal signal before and after learning, proximal weights during learning}}{11}}
\newlabel{fig:echo_state_network_pd_act_grad_desc}{{12}{11}}
\bibcite{OReilly_2014}{{5}{}{{}}{{}}}
\bibcite{Clopath_Bono_2017}{{6}{}{{}}{{}}}
\bibcite{Shai_2015}{{7}{}{{}}{{}}}
\bibcite{Jaeger_2010}{{8}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
